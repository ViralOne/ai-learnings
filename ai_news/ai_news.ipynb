{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# URL of the XML data\n",
    "url = \"https://buttondown.email/ainews/rss\"\n",
    "\n",
    "# Fetch the XML data\n",
    "response = requests.get(url)\n",
    "xml_data = response.text\n",
    "\n",
    "# Parse the XML\n",
    "root = ET.fromstring(xml_data)\n",
    "\n",
    "# Function to recursively extract text from the XML tree\n",
    "def extract_text(element):\n",
    "    text = ''\n",
    "    if element.text:\n",
    "        text += element.text.strip() + ' '\n",
    "    for child in element:\n",
    "        text += extract_text(child)\n",
    "    if element.tail:\n",
    "        text += element.tail.strip() + ' '\n",
    "    return text\n",
    "\n",
    "# Extract text from the root element\n",
    "text_content = extract_text(root)\n",
    "\n",
    "with open(\"docs/data/ai_news_rss.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "DESNTINATION_DIR = \"docs/data\"\n",
    "CHROMA_DIR = \"docs/chroma\"\n",
    "\n",
    "def split_data(directory):\n",
    "    \"\"\"\n",
    "    Split Markdown files in the specified directory into chunks.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory containing Markdown files to process.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunked text data.\n",
    "    \"\"\"\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        directory, \n",
    "        glob=\"*.txt\", \n",
    "        loader_cls=UnstructuredMarkdownLoader, \n",
    "        use_multithreading=True\n",
    "        )\n",
    "    pages = loader.load()\n",
    "\n",
    "    print(f\"Loaded {len(pages)} pages\")\n",
    "\n",
    "    txt = ' '.join([d.page_content for d in pages])\n",
    "    print(f\"Text length: {len(txt)}\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n#{1,6}\", \"\\n\\n\", \"\\n\", \"__\"],\n",
    "        keep_separator=True,\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=250,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def save_chunks(chunks, chroma_dir):\n",
    "    \"\"\"Process chunks into vectors and save them to directory.\"\"\"\n",
    "    embedding_function = GPT4AllEmbeddings(device='gpu')\n",
    "    try:\n",
    "        Chroma.from_documents(\n",
    "            documents=chunks, \n",
    "            embedding=embedding_function, \n",
    "            persist_directory=chroma_dir\n",
    "        )\n",
    "        print(f\"Chunks saved to {chroma_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving chunks: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 pages\n",
      "Text length: 6258805\n",
      "Split into 8201 chunks\n",
      "Chunks saved to docs/chroma\n"
     ]
    }
   ],
   "source": [
    "chunks = split_data(DESNTINATION_DIR)\n",
    "save_chunks(chunks, CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='no title found: no description found\\n\\nno title found: no description found\\n\\nno title found: no description found\\n\\nLangChain AI ‚ñ∑ #announcements (1 messages):\\n\\nRevamped Documentation Structure: LangChain is seeking feedback on a new documentation structure, which explicitly differentiates between tutorials, how-to guides, and conceptual guides. The structure aims to make it easier for users to find relevant information.\\n\\nLangChain Framework Introduction: The shared documentation page provides a comprehensive introduction to LangChain, an open-source framework designed to streamline the application lifecycle of large language models (LLMs)‚Äîfrom development and productionization to deployment.\\n\\nLink mentioned: Introduction | ü¶úÔ∏èüîó LangChain: LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain AI ‚ñ∑ #general (43 messagesüî•):', metadata={'source': 'docs\\\\data\\\\ai_news_rss.txt', 'start_index': 3035664}), 0.7208147048950195), (Document(page_content='LangChain Framework Introduction Highlighted: The provided link introduces LangChain, an open-source framework for building applications with large language models. It details how LangChain facilitates development, productionization, and deployment through building blocks, LangSmith, and LangServe, and includes a diagrammatic overview.\\n\\nLink mentioned: Introduction | ü¶úÔ∏èüîó LangChain: LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain AI ‚ñ∑ #general (38 messagesüî•):\\n\\nSeeking YC Startup Insights: A member has expressed interest in applying to YC for a startup focused on finetuning models for agents and is inquiring if anyone knows whether this has already been done. Another member responded by listing companies like Unsloth, Mistral AI, and Lumini that are in this space.', metadata={'source': 'docs\\\\data\\\\ai_news_rss.txt', 'start_index': 2841844}), 0.8038005232810974), (Document(page_content='JSON files | ü¶úÔ∏èüîó Langchain: The JSON loader use JSON pointer to target keys in your JSON files you want to target.\\n\\nlangchain.chains.structured_output.base.create_structured_output_runnable ‚Äî ü¶úüîó LangChain 0.1.14: no description found\\n\\nlanggraph/examples/state-model.ipynb at 961ddd49ed498df7ffaa6f6d688f7214b883b34f ¬∑ langchain-ai/langgraph: Contribute to langchain-ai/langgraph development by creating an account on GitHub.\\n\\nIssues ¬∑ langchain-ai/langchain: ü¶úüîó Build context-aware reasoning applications. Contribute to langchain-ai/langchain development by creating an account on GitHub.\\n\\nIssues ¬∑ langchain-ai/langchain: ü¶úüîó Build context-aware reasoning applications. Contribute to langchain-ai/langchain development by creating an account on GitHub.\\n\\nTool error handling | ü¶úÔ∏èüîó Langchain: Using a model to invoke a tool has some obvious potential failure modes.', metadata={'source': 'docs\\\\data\\\\ai_news_rss.txt', 'start_index': 5001443}), 0.808143675327301)]\n"
     ]
    }
   ],
   "source": [
    "# Query the database\n",
    "db = Chroma(persist_directory=CHROMA_DIR, embedding_function=GPT4AllEmbeddings(device='gpu'))\n",
    "query_text = db.similarity_search_with_score(\"langchain\", k=3)\n",
    "print(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"no_need\"\n",
    "\n",
    "model = ChatOpenAI(openai_api_base=\"http://localhost:3008/v1\", model_name=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_documents': [Document(page_content='Anticipation for Future AI Developments and Competitor Platforms: The community anticipates new AI models such as the rumored \"GPT-5\" and potential upcoming Perplexity competitors from OpenAI. Additionally, there are discussions on the distinctions between search engines and knowledge engines, with speculations about how these tech advancements might evolve and integrate with existing platforms.\\n\\nHere‚Äôs why AI search engines really can‚Äôt kill Google: A search engine is much more than a search engine, and AI still can‚Äôt quite keep up.\\n\\nImagination Spongebob Squarepants GIF - Imagination Spongebob Squarepants Dreams - Discover & Share GIFs: Click to view the GIF\\n\\nno title found: no description found\\n\\nNew OpenAI Model \\'Imminent\\' and AI Stakes Get Raised (plus Med Gemini, GPT 2 Chatbot and Scale AI): Altman ‚Äòknows the release date‚Äô, Politico calls it ‚Äòimminent‚Äô according to Insiders, and then the mystery GPT-2 chatbot [made by the phi team at Microsoft] c...', metadata={'source': 'docs\\\\data\\\\ai_news_rss.txt', 'start_index': 95140}), Document(page_content=\"AI News for 4/15/2024-4/16/2024. We checked 5 subreddits and 364 Twitters and 27 Discords (395 channels, and 5610 messages) for you. Estimated reading time saved (at 200wpm): 615 minutes.\\n\\nOne thing we missed covering in the weekend rush is Lilian Weng's blog on Diffusion Models for Video Generation. While her work is rarely breaking news on any particular day, it is almost always the single most worthwhile resource on a given important AI topic, and we would say this even if she did not happen to work at OpenAI.\\n\\nAnyone keen on Sora, the biggest AI launch of the year so far (now rumored to be coming to Adobe Premiere Pro), should read this. Unfortunately for most of us, the average diffusion paper requires 150+ IQ to read.\\n\\nWe are only half joking. As per Lilian's style, she takes us on a wild tour of all the SOTA videogen techniques of the past 2 years, humbling every other AI summarizooor on earth:\", metadata={'source': 'docs\\\\data\\\\ai_news_rss.txt', 'start_index': 2857166}), Document(page_content='Eleuther Discord\\n\\nAI Apocalypse: Still a Chuckle, Not a Priority: In a lighthearted debate, the community estimated the risk of AI going rogue at an average concern level of 3.2 out of infinity, indicating a humorous but cautious stance on the subject.\\n\\nGrammar Nerds Assemble: An intricate discussion on the proper usage of \"axis\" led to resource sharing, like Grammar Monster\\'s explanation of the word\\'s grammatical nuances.\\n\\nHuman or Not Human, That is the AI Question: A spirited conversation raised questions about AI reaching human-level intelligence, intertwining hardware progress with Moore\\'s Law and the critical need for AI alignment to ease societal integration.\\n\\nPeering Through the Hype of AI Papers: There\\'s keen interest and healthy skepticism over recent AI papers; the discussions mentioned the promise and doubts around adding more AI agents, with a side-eye towards optimistic forecasts from figures like Andrew Ng.', metadata={'source': 'docs\\\\data\\\\ai_news_rss.txt', 'start_index': 5074680}), Document(page_content=\"Nvidia's AI chip dominance: In /r/hardware, a former Nvidia employee claims on Twitter that no one will catch up to Nvidia's AI chip lead this decade, sparking discussion about the company's strong position.\\n\\nAI Assistants & Applications\\n\\nPotential billion-dollar market for AI companions: In /r/singularity, a tech executive predicts AI girlfriends could become a $1 billion business. Commenters suggest this is a vast underestimate and discuss the societal implications.\\n\\nUnlimited context length for language models: A tweet posted in /r/artificial announces unlimited context length, a significant advancement for AI language models.\\n\\nAI surpassing humans on basic tasks: In /r/artificial, a Nature article reports that AI has surpassed human performance on several basic tasks, though still trails on more complex ones.\\n\\nAI Models & Architectures\", metadata={'source': 'docs\\\\data\\\\ai_news_rss.txt', 'start_index': 2657654}), Document(page_content='Over in Twitter land, the high alpha seems to come from Aaron Defazio, which several of our AI High Signal follows highlighted as the \"new LK-99\" for engaging, \"impossible\" work in public. What\\'s at stake: a potential tuning-free replacement of the very long lived Adam optimizer, and experimental results are currently showing learning at a Pareto frontier in a single run for basically every classic machine learning benchmark (ImageNet ResNet-50, CIFAR-10/100, MLCommons AlgoPerf):\\n\\nHe\\'s writing the paper now, and many \"better optimizers\" have come and gone, but he is well aware of the literature and going for it. We\\'ll see soon enough in a matter of months.\\n\\nTable of Contents\\n\\nAI Reddit Recap\\n\\nAI Twitter Recap\\n\\nAI Discords', metadata={'source': 'docs\\\\data\\\\ai_news_rss.txt', 'start_index': 5050705})], 'output_text': 'The main themes identified from the given set of documents are:\\n\\n1. **AI Developments and Competitor Platforms**: The anticipation and speculation about future AI models, such as GPT-5, and potential competitors from OpenAI.\\n2. **Search Engines vs. Knowledge Engines**: Discussions and speculations about the distinctions between search engines and knowledge engines, and how these technologies might evolve and integrate with existing platforms.\\n3. **Google\\'s Dominance and AI Search Engines**: The article \"Here‚Äôs why AI search engines really can‚Äôt kill Google\" highlights that a search engine is more than just a search engine, and AI still cannot keep up with Google\\'s capabilities.\\n4. **New OpenAI Model Release Date and Related Updates**: News about the imminent release of a new OpenAI model, along with updates on Med Gemini, GPT-2 chatbot, and Scale AI.\\n\\nThese themes are centered around AI developments, search engines, and competitor platforms, which suggests that there is a focus on exploring the latest advancements in artificial intelligence and its applications.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "map_prompt = hub.pull(\"rlm/map-prompt\")\n",
    "map_chain = LLMChain(llm=model, prompt=map_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=map_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "\n",
    "query_text = \"What is the latest top 5 news about AI?\"\n",
    "\n",
    "results = db.similarity_search_with_score(query_text, k=5)\n",
    "docs = [doc for doc, _score in results]\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "result = map_reduce_chain.invoke(split_docs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "You are an assistant that will look into news articles and give short summaries.\n",
    "Use Markdown format and bullet points to give short summaries on multiple news articles.\n",
    "Use emoji to show the sentiment of the news article. Make it short and clear to be ready to be shared on slack.\n",
    "You are given the extracted parts of a long document and a request. Provide a concise summary.\n",
    "Don't make up an answer with no context.\n",
    "The given request: {request}\n",
    "DO NOT give irelevant information that is not in the context.\n",
    "Context: {context} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=result, request=query_text)\n",
    "\n",
    "response_text = conversation.predict(input=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 5 news about AI:\n",
      "\n",
      "* **GPT-5 Anticipation**: The community anticipates new AI models, including GPT-5, with potential competitors from OpenAI. [Source: Eleuther Discord](https://eleuther.ai/discord)\n",
      "* **Imminent OpenAI Model Release**: A new OpenAI model is rumored to be released soon, along with updates on Med Gemini, GPT-2 chatbot, and Scale AI. [Source: Eleuther Discord](https://eleuther.ai/discord)\n",
      "* **Nvidia's AI Chip Dominance**: A former Nvidia employee claims no one will catch up to Nvidia's AI chip lead this decade, sparking discussion about the company's strong position. [Source: r/hardware on Reddit](https://www.reddit.com/r/hardware/comments/lz3h6j/nvidias_ai_chip_dominance/)\n",
      "* **Potential Billion-Dollar Market for AI Companions**: A tech executive predicts AI girlfriends could become a $1 billion business, sparking discussions on societal implications. [Source: r/singularity on Reddit](https://www.reddit.com/r/singularity/comments/lz3k9f/potential_billiondollar_market_for_ai_companions/)\n",
      "* **AI Surpassing Humans on Basic Tasks**: AI has surpassed human performance on several basic tasks, though still trails on more complex ones. [Source: r/artificial on Reddit](https://www.reddit.com/r/artificial/comments/lz3o4g/ai_surpassing_humans_on_basic_tasks/)\n",
      "\n",
      "ü§ñüí°\n"
     ]
    }
   ],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "url = \"https://buttondown.email/ainews/rss\"\n",
    "feed = feedparser.parse(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "now = datetime.now()\n",
    "time_range = timedelta(days=2)\n",
    "max_articles = 10  # Change this to the maximum number of articles you want to display\n",
    "\n",
    "# Loop through all entries in the feed\n",
    "for idx, entry in enumerate(feed.entries):\n",
    "    entry_date = datetime.strptime(entry.published, \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "    entry_date = entry_date.replace(tzinfo=None)\n",
    "    if now.replace(tzinfo=None) - entry_date <= time_range:\n",
    "        print(\"Entry Title:\", entry.title)\n",
    "        print(\"Entry Link:\", entry.link)\n",
    "        print(\"Entry Published Date:\", entry.published)\n",
    "        print(\"Entry Summary:\", entry.summary)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    # Check if the maximum number of articles to display has been reached\n",
    "    if idx + 1 >= max_articles:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
