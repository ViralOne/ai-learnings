{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-openai langchain-community langchainhub gpt4all langchain-chroma chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Handbook\n",
    "# ! git clone https://github.com/AgileFreaks/Handbook.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination directory already exists.\n"
     ]
    }
   ],
   "source": [
    "# Load all files from handbook\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "source_dir = \"handbook\"\n",
    "destination_dir = \"docs/handbook_files\"\n",
    "\n",
    "# Check if the destination directory exists\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)  # Recursive directory creation\n",
    "else:\n",
    "    print(\"Destination directory already exists.\")\n",
    "\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".md\"):\n",
    "            # Construct the source and destination paths\n",
    "            source_path = os.path.join(root, file)\n",
    "            destination_path = os.path.join(destination_dir, file)\n",
    "            \n",
    "            # Move the file to the destination directory\n",
    "            shutil.move(source_path, destination_path)\n",
    "            print(f\"Moved '{source_path}' to '{destination_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entire directory\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "\n",
    "loader = DirectoryLoader(\"E:\\Repos\\LangChain\\docs\\handbook_files\", glob=\"*.md\", loader_cls=UnstructuredMarkdownLoader, use_multithreading=True)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the local server\n",
    "client = ChatOpenAI(base_url=\"http://localhost:3008/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Text from all pages\n",
    "txt = ' '.join([d.page_content for d in pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages:  39\n",
      "Text lenght:  78298\n"
     ]
    }
   ],
   "source": [
    "print(\"Pages: \",len(pages))\n",
    "print(\"Text lenght: \", len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Text into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 250,\n",
    "    add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count chunks\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding with GPT4All\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "embedding_function = GPT4AllEmbeddings(device='gpu')\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=embedding_function,\n",
    "    persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "print(f\"Saved {(len.chunks)} chunks to {persist_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "print(db._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What are the benefits of being a freak?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k is number of results we want to return\n",
    "# docs = db.similarity_search(query_text, k=3)\n",
    "results = db.similarity_search_with_score(query_text, k=3)\n",
    "\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "else:\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {context_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"no_need\"\n",
    "\n",
    "model = ChatOpenAI(openai_api_base=\"http://localhost:3008/v1\", model_name=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Play with this\n",
    "# retriever = vectordb.as_retriever()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "DO NOT give irelevant information that is not in the context.\n",
    "Remember the correct word for the company is AgileFreaks.\n",
    "---\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the question\n",
    "# query_text = \"What are the benefits of being a freak?\"\n",
    "# query_text = \"how much money can i make as a freak?\"\n",
    "# query_text = \"what are the skills and their name, what do they mean?\"\n",
    "query_text = \"how can I get to the next skill level if now I am advanced?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.similarity_search_with_score(query_text, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "else:\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    response_text = conversation.predict(input=prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
